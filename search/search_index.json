{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Siddhi IO Kafka The siddhi-io-kafka extension is an extension to Siddhi that receives and publishes events from and to Kafka. For information on Siddhi and it's features refer Siddhi Documentation . Download Versions 5.x and above with group id io.siddhi.extension.* from here . Versions 4.x and lower with group id org.wso2.extension.siddhi.* from here . Latest API Docs Latest API Docs is 5.0.16 . Features kafka ( Sink ) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. kafka-replay-request ( Sink ) This sink is used to request replay of specific range of events on a specified partition of a topic. kafkaMultiDC ( Sink ) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. kafka ( Source ) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. kafka-replay-response ( Source ) This source is used to listen to replayed events requested from kafka-replay-request sink kafkaMultiDC ( Source ) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Installation For installing this extension in the Streaming Integrator Server, and to add the dependent jars, refer Streaming Integrator documentation section on downloading and installing siddhi extensions .\\ For installing this extension in the Streaming Integrator Tooling, and to add the dependent jars, refer Streaming Integrator documentation section on installing siddhi extensions . Dependencies Following JARs will be converted to osgi and copied to WSO2SI_HOME/lib and WSO2SI_HOME/samples/sample-clients/lib which are in KAFKA_HOME /libs directory. kafka_2.11-*.jar kafka-clients-*.jar metrics-core-*.jar scala-library-2.11.*.jar scala-parser-combinators_2.11.*.jar (if exists) zkclient-*.jar zookeeper-*.jar Setup Kafka As a prerequisite, you have to start the Kafka message broker. Please follow better steps. 1. Download the Kafka distribution 2. Unzip the above distribution and go to the \u2018bin\u2019 directory 3. Start the zookeeper by executing below command, bash zookeeper-server-start.sh config/zookeeper.properties 4. Start the Kafka broker by executing below command, bash kafka-server-start.sh config/server.properties Refer the Kafka documentation for more details, https://kafka.apache.org/quickstart Support and Contribution We encourage users to ask questions and get support via StackOverflow , make sure to add the siddhi tag to the issue for better response. If you find any issues related to the extension please report them on the issue tracker . For production support and other contribution related information refer Siddhi Community documentation.","title":"Information"},{"location":"#siddhi-io-kafka","text":"The siddhi-io-kafka extension is an extension to Siddhi that receives and publishes events from and to Kafka. For information on Siddhi and it's features refer Siddhi Documentation .","title":"Siddhi IO Kafka"},{"location":"#download","text":"Versions 5.x and above with group id io.siddhi.extension.* from here . Versions 4.x and lower with group id org.wso2.extension.siddhi.* from here .","title":"Download"},{"location":"#latest-api-docs","text":"Latest API Docs is 5.0.16 .","title":"Latest API Docs"},{"location":"#features","text":"kafka ( Sink ) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. kafka-replay-request ( Sink ) This sink is used to request replay of specific range of events on a specified partition of a topic. kafkaMultiDC ( Sink ) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. kafka ( Source ) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. kafka-replay-response ( Source ) This source is used to listen to replayed events requested from kafka-replay-request sink kafkaMultiDC ( Source ) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster.","title":"Features"},{"location":"#installation","text":"For installing this extension in the Streaming Integrator Server, and to add the dependent jars, refer Streaming Integrator documentation section on downloading and installing siddhi extensions .\\ For installing this extension in the Streaming Integrator Tooling, and to add the dependent jars, refer Streaming Integrator documentation section on installing siddhi extensions .","title":"Installation"},{"location":"#dependencies","text":"Following JARs will be converted to osgi and copied to WSO2SI_HOME/lib and WSO2SI_HOME/samples/sample-clients/lib which are in KAFKA_HOME /libs directory. kafka_2.11-*.jar kafka-clients-*.jar metrics-core-*.jar scala-library-2.11.*.jar scala-parser-combinators_2.11.*.jar (if exists) zkclient-*.jar zookeeper-*.jar","title":"Dependencies"},{"location":"#setup-kafka","text":"As a prerequisite, you have to start the Kafka message broker. Please follow better steps. 1. Download the Kafka distribution 2. Unzip the above distribution and go to the \u2018bin\u2019 directory 3. Start the zookeeper by executing below command, bash zookeeper-server-start.sh config/zookeeper.properties 4. Start the Kafka broker by executing below command, bash kafka-server-start.sh config/server.properties Refer the Kafka documentation for more details, https://kafka.apache.org/quickstart","title":"Setup Kafka"},{"location":"#support-and-contribution","text":"We encourage users to ask questions and get support via StackOverflow , make sure to add the siddhi tag to the issue for better response. If you find any issues related to the extension please report them on the issue tracker . For production support and other contribution related information refer Siddhi Community documentation.","title":"Support and Contribution"},{"location":"license/","text":"Copyright (c) 2019 WSO2 Inc. ( http://www.wso2.org ) All Rights Reserved. WSO2 Inc. licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ------------------------------------------------------------------------- Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. License shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. Licensor shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. Legal Entity shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, control means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. You (or Your ) shall mean an individual or Legal Entity exercising permissions granted by this License. Source form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. Object form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. Work shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). Derivative Works shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. Contribution shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, submitted means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as Not a Contribution. Contributor shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a NOTICE text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","title":"License"},{"location":"api/4.0.10/","text":"API Docs - v4.0.10 Source kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 STRING Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream. kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . Sink kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"4.0.10"},{"location":"api/4.0.10/#api-docs-v4010","text":"","title":"API Docs - v4.0.10"},{"location":"api/4.0.10/#source","text":"","title":"Source"},{"location":"api/4.0.10/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 STRING Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.10/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.10/#sink","text":"","title":"Sink"},{"location":"api/4.0.10/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.10/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.11/","text":"API Docs - v4.0.11 Source kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream. kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . Sink kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"4.0.11"},{"location":"api/4.0.11/#api-docs-v4011","text":"","title":"API Docs - v4.0.11"},{"location":"api/4.0.11/#source","text":"","title":"Source"},{"location":"api/4.0.11/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.11/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.11/#sink","text":"","title":"Sink"},{"location":"api/4.0.11/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.11/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.12/","text":"API Docs - v4.0.12 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.0.12"},{"location":"api/4.0.12/#api-docs-v4012","text":"","title":"API Docs - v4.0.12"},{"location":"api/4.0.12/#sink","text":"","title":"Sink"},{"location":"api/4.0.12/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.12/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.12/#source","text":"","title":"Source"},{"location":"api/4.0.12/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.12/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.13/","text":"API Docs - v4.0.13 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.0.13"},{"location":"api/4.0.13/#api-docs-v4013","text":"","title":"API Docs - v4.0.13"},{"location":"api/4.0.13/#sink","text":"","title":"Sink"},{"location":"api/4.0.13/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.13/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.13/#source","text":"","title":"Source"},{"location":"api/4.0.13/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.13/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.14/","text":"API Docs - v4.0.14 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.0.14"},{"location":"api/4.0.14/#api-docs-v4014","text":"","title":"API Docs - v4.0.14"},{"location":"api/4.0.14/#sink","text":"","title":"Sink"},{"location":"api/4.0.14/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.14/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.14/#source","text":"","title":"Source"},{"location":"api/4.0.14/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.14/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.15/","text":"API Docs - v4.0.15 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.0.15"},{"location":"api/4.0.15/#api-docs-v4015","text":"","title":"API Docs - v4.0.15"},{"location":"api/4.0.15/#sink","text":"","title":"Sink"},{"location":"api/4.0.15/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.15/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.15/#source","text":"","title":"Source"},{"location":"api/4.0.15/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.15/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.16/","text":"API Docs - v4.0.16 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.0.16"},{"location":"api/4.0.16/#api-docs-v4016","text":"","title":"API Docs - v4.0.16"},{"location":"api/4.0.16/#sink","text":"","title":"Sink"},{"location":"api/4.0.16/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.16/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.16/#source","text":"","title":"Source"},{"location":"api/4.0.16/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.16/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.17/","text":"API Docs - v4.0.17 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.0.17"},{"location":"api/4.0.17/#api-docs-v4017","text":"","title":"API Docs - v4.0.17"},{"location":"api/4.0.17/#sink","text":"","title":"Sink"},{"location":"api/4.0.17/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.17/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.17/#source","text":"","title":"Source"},{"location":"api/4.0.17/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.17/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.7/","text":"API Docs - v4.0.7 Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . Sink kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"4.0.7"},{"location":"api/4.0.7/#api-docs-v407","text":"","title":"API Docs - v4.0.7"},{"location":"api/4.0.7/#source","text":"","title":"Source"},{"location":"api/4.0.7/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.7/#sink","text":"","title":"Sink"},{"location":"api/4.0.7/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.7/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.8/","text":"API Docs - v4.0.8 Source kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 STRING Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream. kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . Sink kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"4.0.8"},{"location":"api/4.0.8/#api-docs-v408","text":"","title":"API Docs - v4.0.8"},{"location":"api/4.0.8/#source","text":"","title":"Source"},{"location":"api/4.0.8/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML and JSON .The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 STRING Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.0.8/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.0.8/#sink","text":"","title":"Sink"},{"location":"api/4.0.8/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.8/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.9/","text":"API Docs - v4.0.9 Sink kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"4.0.9"},{"location":"api/4.0.9/#api-docs-v409","text":"","title":"API Docs - v4.0.9"},{"location":"api/4.0.9/#sink","text":"","title":"Sink"},{"location":"api/4.0.9/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.0.9/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.0.9/#source","text":"","title":"Source"},{"location":"api/4.0.9/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML or JSON format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.0/","text":"API Docs - v4.1.0 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.0"},{"location":"api/4.1.0/#api-docs-v410","text":"","title":"API Docs - v4.1.0"},{"location":"api/4.1.0/#sink","text":"","title":"Sink"},{"location":"api/4.1.0/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.0/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.0/#source","text":"","title":"Source"},{"location":"api/4.1.0/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.0/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.1/","text":"API Docs - v4.1.1 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.1"},{"location":"api/4.1.1/#api-docs-v411","text":"","title":"API Docs - v4.1.1"},{"location":"api/4.1.1/#sink","text":"","title":"Sink"},{"location":"api/4.1.1/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.1/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.1/#source","text":"","title":"Source"},{"location":"api/4.1.1/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.1/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.10/","text":"API Docs - v4.1.10 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.10"},{"location":"api/4.1.10/#api-docs-v4110","text":"","title":"API Docs - v4.1.10"},{"location":"api/4.1.10/#sink","text":"","title":"Sink"},{"location":"api/4.1.10/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.10/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.10/#source","text":"","title":"Source"},{"location":"api/4.1.10/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.10/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.11/","text":"API Docs - v4.1.11 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.11"},{"location":"api/4.1.11/#api-docs-v4111","text":"","title":"API Docs - v4.1.11"},{"location":"api/4.1.11/#sink","text":"","title":"Sink"},{"location":"api/4.1.11/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.11/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.11/#source","text":"","title":"Source"},{"location":"api/4.1.11/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.11/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.12/","text":"API Docs - v4.1.12 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.12"},{"location":"api/4.1.12/#api-docs-v4112","text":"","title":"API Docs - v4.1.12"},{"location":"api/4.1.12/#sink","text":"","title":"Sink"},{"location":"api/4.1.12/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.12/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.12/#source","text":"","title":"Source"},{"location":"api/4.1.12/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.12/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.13/","text":"API Docs - v4.1.13 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.13"},{"location":"api/4.1.13/#api-docs-v4113","text":"","title":"API Docs - v4.1.13"},{"location":"api/4.1.13/#sink","text":"","title":"Sink"},{"location":"api/4.1.13/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.13/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.13/#source","text":"","title":"Source"},{"location":"api/4.1.13/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.13/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.14/","text":"API Docs - v4.1.14 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.14"},{"location":"api/4.1.14/#api-docs-v4114","text":"","title":"API Docs - v4.1.14"},{"location":"api/4.1.14/#sink","text":"","title":"Sink"},{"location":"api/4.1.14/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.14/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.14/#source","text":"","title":"Source"},{"location":"api/4.1.14/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.14/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.15/","text":"API Docs - v4.1.15 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.15"},{"location":"api/4.1.15/#api-docs-v4115","text":"","title":"API Docs - v4.1.15"},{"location":"api/4.1.15/#sink","text":"","title":"Sink"},{"location":"api/4.1.15/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.15/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.15/#source","text":"","title":"Source"},{"location":"api/4.1.15/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.15/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.16/","text":"API Docs - v4.1.16 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.16"},{"location":"api/4.1.16/#api-docs-v4116","text":"","title":"API Docs - v4.1.16"},{"location":"api/4.1.16/#sink","text":"","title":"Sink"},{"location":"api/4.1.16/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.16/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.16/#source","text":"","title":"Source"},{"location":"api/4.1.16/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.16/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.17/","text":"API Docs - v4.1.17 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.17"},{"location":"api/4.1.17/#api-docs-v4117","text":"","title":"API Docs - v4.1.17"},{"location":"api/4.1.17/#sink","text":"","title":"Sink"},{"location":"api/4.1.17/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.17/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.17/#source","text":"","title":"Source"},{"location":"api/4.1.17/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.17/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.18/","text":"API Docs - v4.1.18 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.18"},{"location":"api/4.1.18/#api-docs-v4118","text":"","title":"API Docs - v4.1.18"},{"location":"api/4.1.18/#sink","text":"","title":"Sink"},{"location":"api/4.1.18/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.18/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.18/#source","text":"","title":"Source"},{"location":"api/4.1.18/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive binary events via the Kafka source, this parameter needs to be set to true . false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.18/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.19/","text":"API Docs - v4.1.19 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"4.1.19"},{"location":"api/4.1.19/#api-docs-v4119","text":"","title":"API Docs - v4.1.19"},{"location":"api/4.1.19/#sink","text":"","title":"Sink"},{"location":"api/4.1.19/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.19/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.19/#source","text":"","title":"Source"},{"location":"api/4.1.19/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.19/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.2/","text":"API Docs - v4.1.2 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.2"},{"location":"api/4.1.2/#api-docs-v412","text":"","title":"API Docs - v4.1.2"},{"location":"api/4.1.2/#sink","text":"","title":"Sink"},{"location":"api/4.1.2/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.2/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.2/#source","text":"","title":"Source"},{"location":"api/4.1.2/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.2/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.20/","text":"API Docs - v4.1.20 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"4.1.20"},{"location":"api/4.1.20/#api-docs-v4120","text":"","title":"API Docs - v4.1.20"},{"location":"api/4.1.20/#sink","text":"","title":"Sink"},{"location":"api/4.1.20/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.20/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.20/#source","text":"","title":"Source"},{"location":"api/4.1.20/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.20/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.21/","text":"API Docs - v4.1.21 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"4.1.21"},{"location":"api/4.1.21/#api-docs-v4121","text":"","title":"API Docs - v4.1.21"},{"location":"api/4.1.21/#sink","text":"","title":"Sink"},{"location":"api/4.1.21/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.21/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.21/#source","text":"","title":"Source"},{"location":"api/4.1.21/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.21/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.3/","text":"API Docs - v4.1.3 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.3"},{"location":"api/4.1.3/#api-docs-v413","text":"","title":"API Docs - v4.1.3"},{"location":"api/4.1.3/#sink","text":"","title":"Sink"},{"location":"api/4.1.3/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.3/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.3/#source","text":"","title":"Source"},{"location":"api/4.1.3/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.3/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.4/","text":"API Docs - v4.1.4 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.4"},{"location":"api/4.1.4/#api-docs-v414","text":"","title":"API Docs - v4.1.4"},{"location":"api/4.1.4/#sink","text":"","title":"Sink"},{"location":"api/4.1.4/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.4/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.4/#source","text":"","title":"Source"},{"location":"api/4.1.4/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.4/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.5/","text":"API Docs - v4.1.5 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.5"},{"location":"api/4.1.5/#api-docs-v415","text":"","title":"API Docs - v4.1.5"},{"location":"api/4.1.5/#sink","text":"","title":"Sink"},{"location":"api/4.1.5/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.5/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.5/#source","text":"","title":"Source"},{"location":"api/4.1.5/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.5/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.6/","text":"API Docs - v4.1.6 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.6"},{"location":"api/4.1.6/#api-docs-v416","text":"","title":"API Docs - v4.1.6"},{"location":"api/4.1.6/#sink","text":"","title":"Sink"},{"location":"api/4.1.6/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.6/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.6/#source","text":"","title":"Source"},{"location":"api/4.1.6/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.6/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.7/","text":"API Docs - v4.1.7 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.7"},{"location":"api/4.1.7/#api-docs-v417","text":"","title":"API Docs - v4.1.7"},{"location":"api/4.1.7/#sink","text":"","title":"Sink"},{"location":"api/4.1.7/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.7/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.7/#source","text":"","title":"Source"},{"location":"api/4.1.7/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.7/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.8/","text":"API Docs - v4.1.8 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No topic.offset.map This parameter contains reading offsets for each topic and partition. The parameter should be given in the format topic = offset , topic = offset ,. If the offset is not defined for a certain topic it will read messages from the beginning. e.g., stocks=100,trades=50 will read from 101th message of stocks topic and from 51st message of trades topic null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.8"},{"location":"api/4.1.8/#api-docs-v418","text":"","title":"API Docs - v4.1.8"},{"location":"api/4.1.8/#sink","text":"","title":"Sink"},{"location":"api/4.1.8/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.8/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.8/#source","text":"","title":"Source"},{"location":"api/4.1.8/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No topic.offset.map This parameter contains reading offsets for each topic and partition. The parameter should be given in the format topic = offset , topic = offset ,. If the offset is not defined for a certain topic it will read messages from the beginning. e.g., stocks=100,trades=50 will read from 101th message of stocks topic and from 51st message of trades topic null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.8/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.1.9/","text":"API Docs - v4.1.9 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No topic.offset.map This parameter contains reading offsets for each topic and partition. The parameter should be given in the format topic = offset , topic = offset ,. If the offset is not defined for a certain topic it will read messages from the beginning. e.g., stocks=100,trades=50 will read from 101th message of stocks topic and from 51st message of trades topic null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"4.1.9"},{"location":"api/4.1.9/#api-docs-v419","text":"","title":"API Docs - v4.1.9"},{"location":"api/4.1.9/#sink","text":"","title":"Sink"},{"location":"api/4.1.9/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message To send the binary events via kafka sink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.1.9/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message To send the binary events via kafkaMultiDCSink, it is needed to set this parameter value to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.1.9/#source","text":"","title":"Source"},{"location":"api/4.1.9/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list should beprovided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list should be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message To receive the binary events via kafka source, it is needed to set this parameter value to true . false BOOL Yes No topic.offset.map This parameter contains reading offsets for each topic and partition. The parameter should be given in the format topic = offset , topic = offset ,. If the offset is not defined for a certain topic it will read messages from the beginning. e.g., stocks=100,trades=50 will read from 101th message of stocks topic and from 51st message of trades topic null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.1.9/#kafkamultidc-source","text":"The Kafka Multi Data Center(DC) Source receives records from the same topic in brokers deployed in two different kafka cluster. It will filter out all duplicate messages and try to ensurethat the events are received in the correct order by using sequence numbers. events are received in format such as text , XML JSON and Binary`.The Kafka Source will create the default partition '0' for a given topic, if the topic is not already been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This should contain the kafka server list which the kafka source should be listening to. This should be given in comma separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic The topic which the source would be listening to. eg: 'topic_one' STRING No No partition.no The partition number for the given topic 0 INT Yes No is.binary.message To receive the binary events via KafkaMultiDCSource, it is needed to set this parameter value to true . false BOOL Yes No optional.configuration This may contain all the other possible configurations which the consumer should be created with.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query will listen to 'kafka_topic' topic deployed in broker host1:9092 and host1:9093 with partition 1. There will be a thread created for each broker. The receiving xml events will be mapped to a siddhi event and will be send to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.2.0/","text":"API Docs - v4.2.0 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"4.2.0"},{"location":"api/4.2.0/#api-docs-v420","text":"","title":"API Docs - v4.2.0"},{"location":"api/4.2.0/#sink","text":"","title":"Sink"},{"location":"api/4.2.0/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.2.0/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.2.0/#source","text":"","title":"Source"},{"location":"api/4.2.0/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.2.0/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/4.2.1/","text":"API Docs - v4.2.1 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"4.2.1"},{"location":"api/4.2.1/#api-docs-v421","text":"","title":"API Docs - v4.2.1"},{"location":"api/4.2.1/#sink","text":"","title":"Sink"},{"location":"api/4.2.1/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/4.2.1/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/4.2.1/#source","text":"","title":"Source"},{"location":"api/4.2.1/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/4.2.1/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.0/","text":"API Docs - v5.0.0 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.0"},{"location":"api/5.0.0/#api-docs-v500","text":"","title":"API Docs - v5.0.0"},{"location":"api/5.0.0/#sink","text":"","title":"Sink"},{"location":"api/5.0.0/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.0/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.0/#source","text":"","title":"Source"},{"location":"api/5.0.0/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic-wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.0/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.1/","text":"API Docs - v5.0.1 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.1"},{"location":"api/5.0.1/#api-docs-v501","text":"","title":"API Docs - v5.0.1"},{"location":"api/5.0.1/#sink","text":"","title":"Sink"},{"location":"api/5.0.1/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.1/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.1/#source","text":"","title":"Source"},{"location":"api/5.0.1/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.1/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.10/","text":"API Docs - v5.0.10 Tested Siddhi Core version: 5.1.15 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.10"},{"location":"api/5.0.10/#api-docs-v5010","text":"Tested Siddhi Core version: 5.1.15 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.10"},{"location":"api/5.0.10/#sink","text":"","title":"Sink"},{"location":"api/5.0.10/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.10/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.10/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.10/#source","text":"","title":"Source"},{"location":"api/5.0.10/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.10/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.10/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.11/","text":"API Docs - v5.0.11 Tested Siddhi Core version: 5.1.15 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.11"},{"location":"api/5.0.11/#api-docs-v5011","text":"Tested Siddhi Core version: 5.1.15 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.11"},{"location":"api/5.0.11/#sink","text":"","title":"Sink"},{"location":"api/5.0.11/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.11/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.11/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.11/#source","text":"","title":"Source"},{"location":"api/5.0.11/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.11/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.11/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.12/","text":"API Docs - v5.0.12 Tested Siddhi Core version: 5.1.15 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sync will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.12"},{"location":"api/5.0.12/#api-docs-v5012","text":"Tested Siddhi Core version: 5.1.15 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.12"},{"location":"api/5.0.12/#sink","text":"","title":"Sink"},{"location":"api/5.0.12/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sync will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.12/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.12/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.12/#source","text":"","title":"Source"},{"location":"api/5.0.12/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.12/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.12/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.13/","text":"API Docs - v5.0.13 Tested Siddhi Core version: 5.1.20 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.13"},{"location":"api/5.0.13/#api-docs-v5013","text":"Tested Siddhi Core version: 5.1.20 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.13"},{"location":"api/5.0.13/#sink","text":"","title":"Sink"},{"location":"api/5.0.13/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.13/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.13/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.13/#source","text":"","title":"Source"},{"location":"api/5.0.13/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.13/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.13/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.14/","text":"API Docs - v5.0.14 Tested Siddhi Core version: 5.1.20 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.14"},{"location":"api/5.0.14/#api-docs-v5014","text":"Tested Siddhi Core version: 5.1.20 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.14"},{"location":"api/5.0.14/#sink","text":"","title":"Sink"},{"location":"api/5.0.14/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.14/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.14/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.14/#source","text":"","title":"Source"},{"location":"api/5.0.14/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.14/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.14/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.15/","text":"API Docs - v5.0.15 Tested Siddhi Core version: 5.1.21 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.15"},{"location":"api/5.0.15/#api-docs-v5015","text":"Tested Siddhi Core version: 5.1.21 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.15"},{"location":"api/5.0.15/#sink","text":"","title":"Sink"},{"location":"api/5.0.15/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.15/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.15/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.15/#source","text":"","title":"Source"},{"location":"api/5.0.15/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.15/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.15/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.16/","text":"API Docs - v5.0.16 Tested Siddhi Core version: 5.1.21 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.16"},{"location":"api/5.0.16/#api-docs-v5016","text":"Tested Siddhi Core version: 5.1.21 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.16"},{"location":"api/5.0.16/#sink","text":"","title":"Sink"},{"location":"api/5.0.16/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.16/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/5.0.16/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.16/#source","text":"","title":"Source"},{"location":"api/5.0.16/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.16/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/5.0.16/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.2/","text":"API Docs - v5.0.2 Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.2"},{"location":"api/5.0.2/#api-docs-v502","text":"","title":"API Docs - v5.0.2"},{"location":"api/5.0.2/#sink","text":"","title":"Sink"},{"location":"api/5.0.2/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.2/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.2/#source","text":"","title":"Source"},{"location":"api/5.0.2/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offset.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offset.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.2/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.3/","text":"API Docs - v5.0.3 Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.3"},{"location":"api/5.0.3/#api-docs-v503","text":"Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.3"},{"location":"api/5.0.3/#sink","text":"","title":"Sink"},{"location":"api/5.0.3/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.3/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.3/#source","text":"","title":"Source"},{"location":"api/5.0.3/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.3/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.4/","text":"API Docs - v5.0.4 Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.4"},{"location":"api/5.0.4/#api-docs-v504","text":"Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.4"},{"location":"api/5.0.4/#sink","text":"","title":"Sink"},{"location":"api/5.0.4/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.4/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.4/#source","text":"","title":"Source"},{"location":"api/5.0.4/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.4/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.5/","text":"API Docs - v5.0.5 Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.auto.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.auto.commit This parameter specifies whether to commit offsets automatically. By default, as the Siddhi Kafka source reads messages from Kafka, it will periodically(Default value is set to 1000ms. You can configure it with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . When you would like more control over exactly when offsets are committed, you can set enable.auto.commit to false and Siddhi will commit the offset once the records are successfully processed at the Source. When enable.auto.commit is set to false , manual committing would introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.5"},{"location":"api/5.0.5/#api-docs-v505","text":"Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.5"},{"location":"api/5.0.5/#sink","text":"","title":"Sink"},{"location":"api/5.0.5/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.5/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.5/#source","text":"","title":"Source"},{"location":"api/5.0.5/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.auto.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.auto.commit This parameter specifies whether to commit offsets automatically. By default, as the Siddhi Kafka source reads messages from Kafka, it will periodically(Default value is set to 1000ms. You can configure it with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . When you would like more control over exactly when offsets are committed, you can set enable.auto.commit to false and Siddhi will commit the offset once the records are successfully processed at the Source. When enable.auto.commit is set to false , manual committing would introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.5/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.6/","text":"API Docs - v5.0.6 Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.6"},{"location":"api/5.0.6/#api-docs-v506","text":"Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.6"},{"location":"api/5.0.6/#sink","text":"","title":"Sink"},{"location":"api/5.0.6/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.6/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.6/#source","text":"","title":"Source"},{"location":"api/5.0.6/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.6/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.7/","text":"API Docs - v5.0.7 Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.7"},{"location":"api/5.0.7/#api-docs-v507","text":"Tested Siddhi Core version: 5.1.2 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.7"},{"location":"api/5.0.7/#sink","text":"","title":"Sink"},{"location":"api/5.0.7/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.7/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.7/#source","text":"","title":"Source"},{"location":"api/5.0.7/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.7/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.8/","text":"API Docs - v5.0.8 Tested Siddhi Core version: 5.1.13 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.8"},{"location":"api/5.0.8/#api-docs-v508","text":"Tested Siddhi Core version: 5.1.13 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.8"},{"location":"api/5.0.8/#sink","text":"","title":"Sink"},{"location":"api/5.0.8/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.8/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.8/#source","text":"","title":"Source"},{"location":"api/5.0.8/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream .","title":"kafka (Source)"},{"location":"api/5.0.8/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/5.0.9/","text":"API Docs - v5.0.9 Tested Siddhi Core version: 5.1.13 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sync will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"5.0.9"},{"location":"api/5.0.9/#api-docs-v509","text":"Tested Siddhi Core version: 5.1.13 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.9"},{"location":"api/5.0.9/#sink","text":"","title":"Sink"},{"location":"api/5.0.9/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sync will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/5.0.9/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/5.0.9/#source","text":"","title":"Source"},{"location":"api/5.0.9/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/5.0.9/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"},{"location":"api/latest/","text":"API Docs - v5.0.16 Tested Siddhi Core version: 5.1.21 It could also support other Siddhi Core minor versions. Sink kafka (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute. kafka-replay-request (Sink) This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Sink) A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers Source kafka (Source) A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well kafka-replay-response (Source) This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream kafkaMultiDC (Source) The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"latest"},{"location":"api/latest/#api-docs-v5016","text":"Tested Siddhi Core version: 5.1.21 It could also support other Siddhi Core minor versions.","title":"API Docs - v5.0.16"},{"location":"api/latest/#sink","text":"","title":"Sink"},{"location":"api/latest/#kafka-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to use the Kafka transport, the type parameter should have kafka as its value. Syntax @sink(type= kafka , bootstrap.servers= STRING , topic= STRING , partition.no= INT , sequence.id= STRING , key= STRING , is.binary.message= BOOL , optional.configuration= STRING , is.synchronous= BOOL , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma separated values. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No is.binary.message In order to send the binary events via kafka sink, this parameter is set to 'True'. null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No is.synchronous The Kafka sink will publish the events to the server synchronously when thevalue is set to true , and asynchronously if otherwise false BOOL Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='topic_with_partitions', partition.no='0', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka sink configuration publishes to 0th partition of the topic named topic_with_partitions . EXAMPLE 2 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink( type='kafka', topic='{{symbol}}', partition.no='{{volume}}', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes dynamic topic and partitions that are taken from the Siddhi event. The value for partition.no is taken from the volume attribute, and the topic value is taken from the symbol attribute.","title":"kafka (Sink)"},{"location":"api/latest/#kafka-replay-request-sink","text":"This sink is used to request replay of specific range of events on a specified partition of a topic. Syntax @sink(type= kafka-replay-request , sink.id= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic sink.id a unique SINK_ID should be set. This sink id will be used to match with the appropriate kafka-replay-response source STRING No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-request (Sink)"},{"location":"api/latest/#kafkamultidc-sink","text":"A Kafka sink publishes events processed by WSO2 SP to a topic with a partition for a Kafka cluster. The events can be published in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. The publishing topic and partition can be a dynamic value taken from the Siddhi event. To configure a sink to publish events via the Kafka transport, and using two Kafka brokers to publish events to the same topic, the type parameter must have kafkaMultiDC as its value. Syntax @sink(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , sequence.id= STRING , key= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This parameter specifies the list of Kafka servers to which the Kafka sink must publish events. This list should be provided as a set of comma -separated values. There must be at least two servers in this list. e.g., localhost:9092,localhost:9093 . STRING No No topic The topic to which the Kafka sink needs to publish events. Only one topic must be specified. STRING No No sequence.id A unique identifier to identify the messages published by this sink. This ID allows receivers to identify the sink that published a specific message. null STRING Yes No key The key contains the values that are used to maintain ordering in a Kafka partition. null STRING Yes No partition.no The partition number for the given topic. Only one partition ID can be defined. If no value is specified for this parameter, the Kafka sink publishes to the default partition of the topic (i.e., 0) 0 INT Yes No is.binary.message In order to send the binary events via kafkaMultiDCSink, it is required to set this parameter to true . null BOOL No No optional.configuration This parameter contains all the other possible configurations that the producer is created with. e.g., producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream FooStream (symbol string, price float, volume long); @info(name = 'query1') @sink(type='kafkaMultiDC', topic='myTopic', partition.no='0',bootstrap.servers='host1:9092, host2:9092', @map(type='xml'))Define stream BarStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This query publishes to the default (i.e., 0th) partition of the brokers in two data centers","title":"kafkaMultiDC (Sink)"},{"location":"api/latest/#source","text":"","title":"Source"},{"location":"api/latest/#kafka-source","text":"A Kafka source receives events to be processed by WSO2 SP from a topic with a partition for a Kafka cluster. The events received can be in the TEXT XML JSON or Binary format. If the topic is not already created in the Kafka cluster, the Kafka sink creates the default partition for the given topic. Syntax @source(type= kafka , bootstrap.servers= STRING , topic.list= STRING , group.id= STRING , threading.option= STRING , partition.no.list= STRING , seq.enabled= BOOL , is.binary.message= BOOL , topic.offsets.map= STRING , enable.offsets.commit= BOOL , enable.async.commit= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No topic.list This specifies the list of topics to which the source must listen. This list can be provided as a set of comma-separated values. e.g., topic_one,topic_two STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No partition.no.list The partition number list for the given topic. This is provided as a list of comma-separated values. e.g., 0,1,2, . 0 STRING Yes No seq.enabled If this parameter is set to true , the sequence of the events received via the source is taken into account. Therefore, each event should contain a sequence number as an attribute value to indicate the sequence. false BOOL Yes No is.binary.message In order to receive binary events via the Kafka source,it is required to setthis parameter to 'True'. false BOOL Yes No topic.offsets.map This parameter specifies reading offsets for each topic and partition. The value for this parameter is specified in the following format: topic = offset , topic = offset , When an offset is defined for a topic, the Kafka source skips reading the message with the number specified as the offset as well as all the messages sent previous to that message. If the offset is not defined for a specific topic it reads messages from the beginning. e.g., stocks=100,trades=50 reads from the 101th message of the stocks topic, and from the 51st message of the trades topic. null STRING Yes No enable.offsets.commit This parameter specifies whether to commit offsets. If the manual asynchronous offset committing is needed, enable.offsets.commit should be true and enable.auto.commit should be false . If periodical committing is needed enable.offsets.commit should be true and enable.auto.commit should be true . If committing is not needed, enable.offsets.commit should be false . Note: enable.auto.commit is an optional.configuration property. If it is set to true , Source will periodically(default: 1000ms. Configurable with auto.commit.interval.ms property as an optional.configuration ) commit its current offset (defined as the offset of the next message to be read) for the partitions it is reading from back to Kafka. To guarantee at-least-once processing, we recommend you to enable Siddhi Periodic State Persistence when enable.auto.commit property is set to true . During manual committing, it might introduce a latency during consumption. true BOOL Yes No enable.async.commit This parameter will changes the type of the committing offsets returned on the last poll for the subscribed list of topics and partitions. When enable.async.commit is set to true, committing will be an asynchronous call. true BOOL Yes No optional.configuration This parameter contains all the other possible configurations that the consumer is created with. e.g., ssl.keystore.type:JKS,batch.size:200 . null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic,kafka_topic2', group.id='test', threading.option='partition.wise', bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This kafka source configuration listens to the kafka_topic and kafka_topic2 topics with 0 and 1 partitions. A thread is created for each topic and partition combination. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 2 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source( type='kafka', topic.list='kafka_topic', group.id='test', threading.option='single.thread', bootstrap.servers='localhost:9092', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; This Kafka source configuration listens to the kafka_topic topic for the default partition because no partition.no.list is defined. Only one thread is created for the topic. The events are received in the XML format, mapped to a Siddhi event, and sent to a stream named FooStream . EXAMPLE 3 @App:name('TestExecutionPlan') @source(type='kafka', topic.list='trp_topic', partition.no.list='0', threading.option='single.thread', group.id='group', bootstrap.servers='localhost:9092', @map(type='xml', enclosing.element='//events', @attributes(symbol ='symbol', price = 'price', volume = 'volume', partition = 'trp:partition', topic = 'trp:topic', key = 'trp:key', recordTimestamp = 'trp:record.timestamp', eventTimestamp = 'trp:event.timestamp', checkSum = 'trp:check.sum', topicOffset = 'trp:offset'))) define stream FooStream (symbol string, price float, volume long, partition string, topic string, key string, recordTimestamp string, eventTimestamp string, checkSum string, topicOffset string); from FooStream select * insert into BarStream; This Kafka source configuration listens to the trp_topic topic for the default partition because no partition.no.list is defined. Since the custom attribute mapping is enabled with TRP values, the siddhi event will be populated with the relevant trp values as well","title":"kafka (Source)"},{"location":"api/latest/#kafka-replay-response-source","text":"This source is used to listen to replayed events requested from kafka-replay-request sink Syntax @source(type= kafka-replay-response , bootstrap.servers= STRING , group.id= STRING , threading.option= STRING , sink.id= INT , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This specifies the list of Kafka servers to which the Kafka source must listen. This list can be provided as a set of comma-separated values. e.g., localhost:9092,localhost:9093 STRING No No group.id This is an ID to identify the Kafka source group. The group ID ensures that sources with the same topic and partition that are in the same group do not receive the same event. STRING No No threading.option This specifies whether the Kafka source is to be run on a single thread, or in multiple threads based on a condition. Possible values are as follows: single.thread : To run the Kafka source on a single thread. topic.wise : To use a separate thread per topic. partition.wise : To use a separate thread per partition. STRING No No sink.id a unique SINK_ID . INT No No Examples EXAMPLE 1 @App:name('TestKafkaReplay') @sink(type='kafka-replay-request', sink.id='1') define stream BarStream (topicForReplay string, partitionForReplay string, startOffset string, endOffset string); @info(name = 'query1') @source(type='kafka-replay-response', group.id='group', threading.option='single.thread', bootstrap.servers='localhost:9092', sink.id='1', @map(type='json')) Define stream FooStream (symbol string, amount double); @sink(type='log') Define stream logStream(symbol string, amount double); from FooStream select * insert into logStream; In this app we can send replay request events into BarStream and observe the replayed events in the logStream","title":"kafka-replay-response (Source)"},{"location":"api/latest/#kafkamultidc-source","text":"The Kafka Multi-Datacenter(DC) source receives records from the same topic in brokers deployed in two different kafka clusters. It filters out all the duplicate messages and ensuresthat the events are received in the correct order using sequential numbering. It receives events in formats such as TEXT , XML JSON and Binary`.The Kafka Source creates the default partition '0' for a given topic, if the topic has not yet been created in the Kafka cluster. Syntax @source(type= kafkaMultiDC , bootstrap.servers= STRING , topic= STRING , partition.no= INT , is.binary.message= BOOL , optional.configuration= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic bootstrap.servers This contains the kafka server list which the kafka source listens to. This is given using comma-separated values. eg: 'localhost:9092,localhost:9093' STRING No No topic This is the topic that the source listens to. eg: 'topic_one' STRING No No partition.no This is the partition number of the given topic. 0 INT Yes No is.binary.message In order to receive the binary events via the Kafka Multi-DC source, the value of this parameter needs to be set to 'True'. false BOOL Yes No optional.configuration This contains all the other possible configurations with which the consumer can be created.eg: producer.type:async,batch.size:200 null STRING Yes No Examples EXAMPLE 1 @App:name('TestExecutionPlan') define stream BarStream (symbol string, price float, volume long); @info(name = 'query1') @source(type='kafkaMultiDC', topic='kafka_topic', bootstrap.servers='host1:9092,host1:9093', partition.no='1', @map(type='xml')) Define stream FooStream (symbol string, price float, volume long); from FooStream select symbol, price, volume insert into BarStream; The following query listens to 'kafka_topic' topic, deployed in the broker host1:9092 and host1:9093, with partition 1. A thread is created for each broker. The receiving xml events are mapped to a siddhi event and sent to the FooStream.","title":"kafkaMultiDC (Source)"}]}